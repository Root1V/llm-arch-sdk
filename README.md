# LLM Arch SDK

SDK para consumir llama-server con autenticaciÃ³n y renovaciÃ³n automÃ¡tica de tokens.

## DescripciÃ³n

Este SDK proporciona una interfaz unificada para interactuar con servidores LLM (como llama-server), manejando autenticaciÃ³n, renovaciÃ³n de tokens, circuit breakers y diferentes adaptadores para proveedores como OpenAI y Llama.

## CaracterÃ­sticas

- **AutenticaciÃ³n automÃ¡tica**: Manejo de tokens con renovaciÃ³n automÃ¡tica.
- **TokenManager opcional**: Crea automÃ¡ticamente una instancia si no se proporciona.
- **Circuit Breaker**: ProtecciÃ³n contra fallos en las llamadas a la API.
- **Adaptadores mÃºltiples**: Soporte para Llama, OpenAI y LangChain (ChatOpenAI).
- **NormalizaciÃ³n de respuestas**: EstandarizaciÃ³n de respuestas de diferentes proveedores.
- **Cliente HTTP robusto**: Uso de httpx con configuraciones personalizables.
- **Ejemplos con .env**: Los ejemplos cargan variables desde archivo `.env` usando python-dotenv.

## InstalaciÃ³n

### Requisitos

- Python >= 3.14

### InstalaciÃ³n desde cÃ³digo fuente

1. Clona el repositorio:
   ```bash
   git clone https://github.com/Root1V/llm-arch-sdk.git
   cd llm_arch_sdk
   ```

2. Instala las dependencias:
   ```bash
   uv sync
   uv add --dev pytest 
   ```

3. Activa el entorno virtual:
   ```bash
   source .venv/bin/activate
   ```

4. Instala el paquete en modo editable (para que ejecuten los test)
   ```bash
   pip install -e .
   ```

5. Build del paquete
   ```bash
   uv build
   ```

### InstalaciÃ³n en el cliente desde repositorio local

1. Clona el respoitorio en la version que requieras
```
git fetch --tags && git checkout v0.3.0
```

2. Crea el paquete del sdk
```
uv build
```

3. Copia el sdk compilado a la carpeta de repo (opcional)
```
cp /llm_arch_sdk/dist/llm_arch_sdk-0.3.0* /opt/python-repo/
```

4. Agrega el sdk en tu proyecto y sincroniza las dependencias
```
uv add --find-links /opt/python-repo/ llm-arch-sdk
uv sync --find-links /opt/python-repo/
```

5. Otra alternativa de instalacion usando "pip"
```
pip install --find-links=/opt/python-repo llm_arch_sdk
```

## Ejemplos de uso

La carpeta `examples/` contiene scripts demostrativos para probar las funcionalidades del SDK.

**Nota importante**: Los ejemplos cargan las variables de entorno desde un archivo `.env`. El SDK automatiza la carga usando `python-dotenv`, asÃ­ que no necesitas escribir las credenciales en el cÃ³digo.

### ConfiguraciÃ³n de autenticaciÃ³n

Crea un archivo `.env` en la carpeta `examples/`:

```
LLM_BASE_URL=http://localhost:8080
LLM_USERNAME=tu_usuario
LLM_PASSWORD=tu_contraseÃ±a
```

### Ejecutar ejemplos

#### Ejemplo bÃ¡sico con Llama
```bash
uv run python examples/basic_usage.py
```

#### Ejemplo con OpenAI
```bash
uv run python examples/openai_example.py
```

#### Ejemplo con LangChain
```bash
uv run python examples/langchain_example.py
```

Estos ejemplos incluyen manejo de errores y funcionan tanto con servidores reales como con configuraciones de prueba.

## Estructura del Proyecto

```
llm_arch_sdk/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llm_arch_sdk/
â”‚       â”œâ”€â”€ adapters/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py
â”‚       â”‚   â”œâ”€â”€ lang_adapter.py
â”‚       â”‚   â”œâ”€â”€ llama_adapter.py
â”‚       â”‚   â””â”€â”€ open_ai_adapter.py
â”‚       â”œâ”€â”€ auth/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ token_manager.py
â”‚       â”œâ”€â”€ client/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base_client.py
â”‚       â”‚   â”œâ”€â”€ chat_completions.py
â”‚       â”‚   â”œâ”€â”€ completions.py
â”‚       â”‚   â”œâ”€â”€ embeddings.py
â”‚       â”‚   â””â”€â”€ llm_client.py
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ chat_completion.py
â”‚       â”‚   â”œâ”€â”€ completion.py
â”‚       â”‚   â”œâ”€â”€ generation_settings.py
â”‚       â”‚   â”œâ”€â”€ llm_response.py
â”‚       â”‚   â”œâ”€â”€ stop_type.py
â”‚       â”‚   â”œâ”€â”€ timings.py
â”‚       â”‚   â””â”€â”€ usage.py
â”‚       â”œâ”€â”€ normalizers/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ completion_detector.py
â”‚       â”‚   â””â”€â”€ content_normalizer.py
â”‚       â”œâ”€â”€ observability/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ helpers.py
â”‚       â”‚   â”œâ”€â”€ langfuse_client.py
â”‚       â”‚   â””â”€â”€ masking.py
â”‚       â””â”€â”€ transport/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ auth_http_client_factory.py
â”‚           â”œâ”€â”€ circuit_breaker.py
â”‚           â””â”€â”€ http_client_factory.py
â”œâ”€â”€ main.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ uv.lock
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

### DescripciÃ³n de mÃ³dulos

- **adapters/**: Adaptadores para diferentes proveedores de LLM (OpenAI, Llama).
- **auth/**: GestiÃ³n de autenticaciÃ³n y tokens.
- **client/**: Cliente principal y endpoints especÃ­ficos (chat, completions, embeddings).
- **models/**: Modelos de datos para respuestas y configuraciones.
- **normalizers/**: Utilidades para normalizar respuestas.
- **transport/**: Manejo de transporte HTTP, circuit breakers y fÃ¡bricas de clientes.

## Pruebas

Para ejecutar las pruebas:

```bash
uv run pytest test/
```

El proyecto incluye 90 pruebas unitarias organizadas en una estructura que refleja el cÃ³digo fuente, facilitando el mantenimiento y la localizaciÃ³n de tests relacionados con mÃ³dulos especÃ­ficos.

### Estructura de pruebas

- `test/client/`: Tests para clientes y endpoints (chat, completions, embeddings)
- `test/auth/`: Tests para autenticaciÃ³n y gestiÃ³n de tokens
- `test/transport/`: Tests para circuit breaker y transporte HTTP
- `test/adapters/`: Tests para adaptadores de proveedores (Llama, OpenAI)
- `test/models/`: Tests para modelos de datos y parsing JSON
- `test/normalizers/`: Tests para normalizaciÃ³n de contenido

### Cobertura de pruebas

- **Cobertura de pruebas**: 90 tests unitarios
- **TokenManager**: AutenticaciÃ³n, renovaciÃ³n de tokens, circuit breaker.
- **CircuitBreaker**: Estados CLOSED/OPEN/HALF_OPEN, timeouts.
- **Clientes**: ChatCompletions, Completions, Embeddings.
- **Adaptadores**: LlamaAdapter, OpenAIAdapter, LangChainAdapter.
- **Modelos**: Parsing de respuestas JSON, validaciÃ³n de datos.
- **Normalizadores**: DetecciÃ³n de completitud semÃ¡ntica, limpieza de texto.
- **Transporte**: Manejo de HTTP, errores, timeouts.

## Historial de cambios

### v0.4.0 (En desarrollo)
- ğŸš€ Nuevo adaptador LangChainAdapter para integraciÃ³n con LangChain
- ğŸ“ Soporte para ChatOpenAI de LangChain
- âœ… 7 nuevos tests unitarios para LangChainAdapter (90 tests totales)
- ğŸ”„ PatrÃ³n **kwargs implementado en todos los adaptadores
- ğŸ“š Nuevo ejemplo: `examples/langchain_example.py`

### v0.3.0
- âœ… TokenManager ahora es **opcional** en `AuthHttpClientFactory.create()`
- âœ… Se crea automÃ¡ticamente una instancia si no se proporciona
- âœ… Ejemplos actualizados para usar `.env` con `python-dotenv`
- âœ… Todos los 83 tests pasan correctamente

### v0.2.0
- ğŸ”§ Refactor: ConsolidaciÃ³n de manejo de headers y mejora de herencia en HTTP client factories
- Mejora de la arquitectura del transporte

### v0.1.0
- ğŸ‰ Release inicial del LLM Arch SDK
- âœ… AutenticaciÃ³n automÃ¡tica con TokenManager
- âœ… Circuit Breaker para protecciÃ³n contra fallos
- âœ… Adaptadores para Llama y OpenAI
- âœ… Cliente HTTP robusto con httpx
- âœ… NormalizaciÃ³n de respuestas
- âœ… 83 tests unitarios
- âœ… DocumentaciÃ³n y ejemplos de uso

## ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## Autor

Emeric Espiritu Santiago
