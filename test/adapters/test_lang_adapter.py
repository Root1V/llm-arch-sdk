import pytest
import os
from unittest.mock import Mock, patch, ANY
from langchain_openai import ChatOpenAI
from llm_arch_sdk.adapters.lang_adapter import LangChainAdapter
from llm_arch_sdk.auth.token_manager import TokenManager


class TestLangChainAdapter:
    @patch.dict(os.environ, {'LLM_BASE_URL': 'http://localhost:8000', 'LLM_USERNAME': 'test', 'LLM_PASSWORD': 'test'}, clear=True)
    @patch.object(TokenManager, '_login')
    def test_init_with_env_var(self, mock_login):
        mock_login.return_value = None
        adapter = LangChainAdapter()

        assert adapter.base_url == 'http://localhost:8000'
        assert adapter.timeout == 60.0
        assert adapter.client_kwargs == {}

    def test_init_with_custom_params(self):
        adapter = LangChainAdapter(
            base_url="http://custom:9000",
            timeout=30.0,
            model="gpt-4",
            temperature=0.5
        )

        assert adapter.base_url == "http://custom:9000"
        assert adapter.timeout == 30.0
        assert adapter.client_kwargs == {
            "model": "gpt-4",
            "temperature": 0.5
        }

    @patch.dict(os.environ, {}, clear=True)
    def test_init_missing_base_url(self):
        with pytest.raises(RuntimeError, match="LLM_BASE_URL no está configurado"):
            LangChainAdapter()

    @patch.dict(os.environ, {'LLM_BASE_URL': 'invalid-url'}, clear=True)
    def test_init_invalid_base_url(self):
        with pytest.raises(RuntimeError, match="LLM_BASE_URL inválida: invalid-url"):
            LangChainAdapter()

    @patch.dict(os.environ, {'LLM_BASE_URL': 'ftp://invalid.com'}, clear=True)
    def test_init_non_http_base_url(self):
        with pytest.raises(RuntimeError, match="LLM_BASE_URL inválida: ftp://invalid.com"):
            LangChainAdapter()

    @patch.dict(os.environ, {'LLM_BASE_URL': 'http://localhost:8000', 'LLM_USERNAME': 'test', 'LLM_PASSWORD': 'test'}, clear=True)
    @patch.object(TokenManager, '_login')
    @patch('llm_arch_sdk.adapters.lang_adapter.ChatOpenAI')
    def test_client_lazy_initialization(self, mock_chatOpenai_class, mock_login):
        mock_login.return_value = None
        mock_chatOpenai_instance = Mock(spec=ChatOpenAI)
        mock_chatOpenai_class.return_value = mock_chatOpenai_instance

        adapter = LangChainAdapter()
        client1 = adapter.client()
        client2 = adapter.client()

        # Should return the same instance
        assert client1 is mock_chatOpenai_instance
        assert client2 is mock_chatOpenai_instance

        # ChatOpenAI should be instantiated only once
        assert mock_chatOpenai_class.call_count == 1
        # Check that key arguments were passed
        call_kwargs = mock_chatOpenai_class.call_args[1]
        assert call_kwargs['base_url'] == 'http://localhost:8000'
        assert call_kwargs['api_key'] == 'unused'

    @patch.dict(os.environ, {'LLM_BASE_URL': 'http://custom.api.com', 'LLM_USERNAME': 'test', 'LLM_PASSWORD': 'test'}, clear=True)
    @patch.object(TokenManager, '_login')
    @patch('llm_arch_sdk.adapters.lang_adapter.ChatOpenAI')
    def test_client_initialization_params(self, mock_chatOpenai_class, mock_login):
        mock_login.return_value = None
        mock_chatOpenai_instance = Mock(spec=ChatOpenAI)
        mock_chatOpenai_class.return_value = mock_chatOpenai_instance

        adapter = LangChainAdapter(
            base_url="http://test.api.com",
            timeout=45.0,
            model="gpt-4",
            temperature=0.3
        )
        adapter.client()

        # Check that ChatOpenAI was called with correct parameters
        assert mock_chatOpenai_class.call_count == 1
        call_kwargs = mock_chatOpenai_class.call_args[1]
        assert call_kwargs['base_url'] == "http://test.api.com"
        assert call_kwargs['api_key'] == 'unused'
        assert call_kwargs['model'] == "gpt-4"
        assert call_kwargs['temperature'] == 0.3
